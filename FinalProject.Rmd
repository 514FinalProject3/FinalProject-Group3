---
title: "FinalProject"
author: "Group 3"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tigerstats)
library(readr)
library(tidyverse)
require(webr)
```

## Section 1 - Loading the data and Exploring the data

We have selected Bike dataset to perform exploration of the data. We start by loading the data into the R Markdown file

```{r}
bike_data <- read_csv("Cleaned_bike_data.csv",show_col_types = FALSE)
```


```{r}
summary(bike_data)
```

> We can see that there are different columns in the table which are Model_name, Model_year, Kms_driven, Owner, Location, Mileage, Power and Price.

### To determine the different types of the datatypes, we can use the str command in R. This command tells us the different types of data which is present in the datasheet.

```{r}
str(bike_data)
```

> The different types of datatypes which are present in the dataset are numeric and charachter.  

### To determine the null values in the table, we can use the built in method in R. 

```{r}
null_values <- sum(is.na(bike_data)) 
null_values
```

> With the above method, we can see that there are 0 missing/Null values in the dataset.

### Using built in methods to check for duplicates

```{r}
duplicate_values <- sum(duplicated(bike_data))
duplicate_values
```

> There are 2 duplicate values in this dataset, that we will remove.

### Removing duplicates

```{r}
bike_data <- bike_data[!duplicated(bike_data),]
```


## Part 2 - Graphical Overview

```{r pressure, echo=FALSE}
hist(bike_data$model_year, main = "Frequency of Bikes per Model Year", xlab = "Model Year", ylab = "Frequency")
```

> This graph indicates the frequency of bikes by their model year. While most of the motorcycles were manufactured in from 2010-2019, there are some from 2000-2009, and 2020 and onwards as well. There are also some preceding the year 2000, and while we cannot ignore them, those numbers are incredibly minute and non-discernible.

### Standardizing some values and checking for outliers:

```{r}
standard_bikePower <- scale(bike_data$power)
par(mfrow = c(1,2))
boxplot(standard_bikePower)
hist(bike_data$power)
```

> These graphs show us that outliers do indeed exist in this data. The boxplot (of the scaled data) indicates that all of the outliers are high and exist beyond Q3, and also shows that the interquartile range is small, and that Q1 and Q3 are as well.
The histogram (not scaled) indicates the true values of the power of these bikes, and we can immediately see that the majority of them have a power of under 50, with over 2500 falling into the 20-29 range alone. However, there are still some small numbers of bikes that exceed that, some with a power of close to 200.

### Extracting brand names from model names column to be used in later graphs

```{r}
brand_model <- bike_data$model_name 
testing <- str_split_fixed(brand_model, " ", 2)
brand_name <- testing[,1]
brand_name = str_replace_all(brand_name, "Royal", "Royal Enfield")
brand_df <- data.frame(brand_name)

bike_data_new <- cbind(bike_data, brand_df)
```


### Bar plot showing relationship between brands and number of owners

```{r}
ggplot(data = bike_data_new) +
  geom_bar(mapping = aes(fill = brand_name, x = owner))
```

> "Owner" is a discrete category found in bike_data and "brand_name" is a discrete category that has been appended to the data by extracting existing values.
This barplot illustrates the relationship between the brand/make of the bikes and how many owners they've had. While the categories for "third owner" and "fourth owner or more" are too small to be discernible, this plot shows us that Royal Enfield, Bajaj, Honda and Yamaha are the most popular brands for bikes for both first and second owners, which could potentially be attributed to their longevity.


## Part 3 - Hypothesis Testing
This part we gonna discuss about Hypothesis testing in order to see if it has meaningful results. We did three hypothesis testing. All of these focus on bike’s price. 
```{r }
data <- read.csv("Cleaned_bike_data.csv")
str(data)
```



### First hand owner price higher the Second hand owner (One-side test)
Let’s begin with testing First hand owner price and the Second hand owner, the hypothesis is the first hand price should higher. Therefore, H0 is the first owner price and second owner price are equal. The alternative is first owner price will higher that the second hand price. 

\begin{align*}
H_0: \mu_{first\_owner\_price}-\mu_{second\_owner\_price} & = 0 \\
H_1: \mu_{first\_owner\_price}-\mu_{second\_owner\_price} & > 0 
\end{align*}

```{r , fig.width=8, fig.height=7}


#split first hand owner and second hand owner
first_hand_owner <- subset(data, owner=="first owner")
second_hand_owner <- subset(data, owner=="second owner")


first_second_owner <- rbind(first_hand_owner, second_hand_owner)


ggplot(first_second_owner, aes(x=owner, y=price, fill=owner)) + 
    geom_boxplot(alpha=0.8) +
    theme(legend.position="none") +
    scale_fill_brewer(palette="Dark2") +
    coord_flip()
```
\

>Check boxplot and we can see that there are many extreme price for first hand owners and the both median price look no different.

 \
Don't know variance. So, use T-testing but need to check variance whether or not equal. \
Check variance equal or not \

\begin{align*}
H_0: \sigma_{first\_owner\_price}^2 = \sigma_{second\_owner\_price}^2 \\
H_1:  \sigma_{first\_owner\_price}^2 \ne \sigma_{second\_owner\_price}^2 
\end{align*}

```{r , fig.width=8, fig.height=7}
var.test(first_hand_owner$price, second_hand_owner$price, alternative = "two.sided")
#Not reject H0 => variance equal

#t-testing
t.test(first_hand_owner$price, second_hand_owner$price,alternative="greater", var.equal = TRUE, conf.level = 0.95)

plot(t.test(first_hand_owner$price, second_hand_owner$price,alternative="greater", var.equal = TRUE, conf.level = 0.95))

```
\
>Due to we don’t have variance, we have to choose T-testing. However, we need to check the variance between two groups first whether it equal or not.  P value greater than .05 So, we should not reject H0. It means the variance of two groups are equal.  Then do one-side t testing, the result of the testing P value greater than  .05. It means that we should not reject H0. It indicates that we can accept that first hand owner price not different from the second hand owner price.


### Price of second hand bike not be different from third and later price (Two-side test) \
The second testing, we will check whether or not Price of Second hand bike not be different from third and later hand. 

\begin{align*}
H_0: \mu_{second\_owner\_price}-\mu_{later\_second\_owner\_price} & = 0 \\
H_1: \mu_{second\_owner\_price}-\mu_{later\_second\_owner\_price} & \ne 0 
\end{align*}


```{r, fig.width=8, fig.height=7}
second_hand_owner <- subset(data, owner=="second owner")
later_second_hand_owner <- subset(data, owner =="third owner" | owner == "fourth owner or more")

later_second_hand_owner$owner[later_second_hand_owner$owner == "third owner" | later_second_hand_owner$owner == "fourth owner or more"]<- "third owner or more"

second_later_owner <- rbind(second_hand_owner, later_second_hand_owner)


ggplot(second_later_owner, aes(x=owner, y=price, fill=owner)) + 
    geom_boxplot(alpha=0.8) +
    theme(legend.position="none") +
    scale_fill_brewer(palette="Dark2") +
    coord_flip()
```
\

>After split the data into two group, we created a boxplot.  We can see that the second owners has more outliers and the the range larger. 

Don't know variance. So, use T-testing but need to check variance whether or not equal. \
Check variance equal or not \
\begin{align*}
H_0: \sigma_{second\_owner\_price}^2 = \sigma_{later\_second\_owner\_price}^2 \\
H_1:  \sigma_{second\_owner\_price}^2 \ne \sigma_{later\_second\_owner\_price}^2 
\end{align*}

```{r, fig.width=8, fig.height=7}
var.test(second_hand_owner$price, later_second_hand_owner$price, alternative = "two.sided")
#reject H0 => variance not equal

#t-testing
t.test(second_hand_owner$price, later_second_hand_owner$price,alternative="two.sided", var.equal = FALSE, conf.level = 0.95)

plot(t.test(second_hand_owner$price, later_second_hand_owner$price,alternative="two.sided", var.equal = FALSE, conf.level = 0.95))
```

> Due to we don’t have variance, we have to choose T-testing. However, we need to check the variance between two groups first whether it equal or not.  P value less than .05 So, we should reject H0. It means the variance of two groups are not equal.  Then do two-side t testing, the result of the testing P value less than  .05. It means that we should reject H0. It indicates that we can accept that second hand owner price is different from the later owner price.


### Higher kilometers driven price cheaper than lower kilometers driven price (One side test)
The last hypothesis testing, we will test on the hypothesis about the price of two group which was splited by the median of kilometers. The group that has higher kilometers driven will cheaper than the group that has lower kilometers driven.

\begin{align*}
H_0: \mu_{greater\_kms\_driven\_price}-\mu_{less\_kms\_driven\_price} & = 0 \\
H_1: \mu_{greater\_kms\_driven\_price}-\mu_{less\_kms\_driven\_price} & < 0
\end{align*}

```{r,  fig.width=8, fig.height=7}
median(data$kms_driven)


greater_median_kms_driven <- subset(data, kms_driven > 18000)
less_median_kms_driven <- subset(data, kms_driven <= 18000)

greater_median_kms_driven["median_kms_driven"] <- "greater"
less_median_kms_driven["median_kms_driven"] <- "less"

all_kms_driven <- rbind(greater_median_kms_driven, less_median_kms_driven)


ggplot(all_kms_driven, aes(x=median_kms_driven, y=price, fill=median_kms_driven)) + 
    geom_boxplot(alpha=0.8) +
    theme(legend.position="none") +
    scale_fill_brewer(palette="Dark2") +
    coord_flip()

```
\

>The boxplot shows that the group that has less kms driven has higher price.

Don't know variance. So, use T-testing but need to check variance whether or not equal.
Check variance equal or not
\begin{align*}
H_0: \sigma_{greater\_median\_kms\_driven\_price}^2 = \sigma_{less\_median\_kms\_driven\_price}^2 \\
H_1:  \sigma_{greater\_median\_kms\_driven\_price}^2 \ne \sigma_{less\_median\_kms\_driven\_price}^2 
\end{align*}

```{r, fig.width=8, fig.height=7}
var.test(greater_median_kms_driven$price, less_median_kms_driven$price , alternative = "two.sided")
# reject H0 => variance not equal

#t-testing
t.test(greater_median_kms_driven$price, less_median_kms_driven$price,alternative="less", var.equal = FALSE, conf.level = 0.95)

plot(t.test(greater_median_kms_driven$price, less_median_kms_driven$price,alternative="less", var.equal = FALSE, conf.level = 0.95))
```
\
> Due to we don’t have variance, we have to choose T-testing. However, we need to check the variance between two groups first whether it equal or not.  P value less than .05 So, we should reject H0. It means the variance of two groups are not equal.  Then do one-side t testing, the result of the testing P value much less than  .05. It means that we should reject H0. It would mean that we can accept that the group that has greater kilometer driven will cheaper than less kilometer driven.



## Part 4 - Linear Regression

This section is for Linear Regression.

Including the necessary packages for regression
```{r}
library(MASS)
library(MLmetrics)
```

Setting the seed to store the data and keep it same.

```{r}
set.seed(101)
```

Splitting the Dataset into Training and Testing Dataset randomly. We have splitted the data in 70% and 30%.

```{r}
i = sample(2, nrow(bike_data), replace=TRUE, prob=c(0.7, 0.3))
bikeTraining <- bike_data[i==1,]
bikeTesting <- bike_data[i==2,]
```
The Training Dataset consists of 3554 entries and the Testing dataset consists of 1508 entries. 

# Model 1 - Forward Propogation Model.

We start with constructing the intercept model. The intercept is used to form a linear regression model with a constant variable. The full model is used to select all the attributes which are seen in the data table. We then use the stepAIC function to travel step by step and select all the elements with the highest AIC until the occurrence of the null variable. 

```{r}
intercept_model <- lm(price ~ 1, data = bikeTraining[,1:8])
full_model <- lm(price ~ .-model_name, data = bikeTraining[,1:8])
forward_model <- stepAIC(intercept_model, direction = "forward",scope = formula(full_model))
```

As we can see that the full model is constructed without the use of the model name. The reason for not including the model name is that it is not revelant to the price. 


```{r}
summary(forward_model)
forward_model$anova
```

### We can calculate the MAE and MSE for both the backward model as follows:

```{r}
forward_pred <-predict(object = forward_model, newdata = bikeTesting[,1:8])
MAE(y_pred = forward_pred, y_true = bikeTesting$price)
MSE(y_pred = forward_pred, y_true = bikeTesting$price)
```

### Plotting the Forward Propogation Model

```{r}
par(mfrow=c(2,2))
plot(forward_model)
```

# Model 2 - Backward Propogation

```{r}
backward <- stepAIC(full_model, direction = "backward")
summary(backward)
```

> We see that the accuracy of the backward model is similar to that of the forward model. 

### Calculation of the MAE and MSE for the backward model 

```{r}
backward_pred <-predict(object = backward, newdata = bikeTesting[,1:8])
MAE(y_pred = backward_pred, y_true = bikeTesting$price)
MSE(y_pred = backward_pred, y_true = bikeTesting$price)
```

### Plotting the Backward Propogation model 

```{r}
par(mfrow = c(2,2))
plot(backward)
```


> We can see that both the models have the same accuracy and same mean square error and Mean absolute error. The accuracy for both the models is 77.02%.
