---
title: "FinalProject"
author: "Group 3"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tigerstats)
library(readr)
library(tidyverse)
#library(webr)
```

## Section 1 - Loading the data and Exploring the data

We have selected Bike dataset to perform exploration of the data. We start by loading the data into the R Markdown file

```{r}
bike_data <- read_csv("Cleaned_bike_data.csv",show_col_types = FALSE)
```


```{r}
summary(bike_data)
```

We can see that there are different columns in the table which are Model_name, Model_year, Kms_driven, Owner, Location, Mileage, Power and Price.

To determine the different types of the datatypes, we can use the str command in R. This command tells us the different types of data which is present in the datasheet.
```{r}
str(bike_data)
```

The different types of datatypes which are present in the dataset are numeric and charachter.  

To determine the null values in the table, we can use the built in method in R. 

```{r}
null_values <- sum(is.na(bike_data)) 
null_values
```

With the above method, we can see that there are 0 missing/Null values in the dataset.

## Part 2 - Graphical Overview

```{r pressure, echo=FALSE}
hist(bike_data$model_year, main = "Frequency of Bikes per Model Year", xlab = "Model Year", ylab = "Frequency")
```

> This graph indicates the frequency of bikes by their model year. While most of the motorcycles were manufactured in from 2010-2019, there are some from 2000-2009, and 2020 and onwards as well. There are also some preceding the year 2000, and while we cannot ignore them, those numbers are incredibly minute and non-discernible.

Standardizing some values and checking for outliers:

```{r}
standard_bikePower <- scale(bike_data$power)
boxplot(standard_bikePower)
hist(bike_data$power)
```

> These graphs show us that outliers do indeed exist in this data. The boxplot (of the scaled data) indicates that all of the outliers are high and exist beyond Q3, and also shows that the interquartile range is small, and that Q1 and Q3 are as well.
The histogram (not scaled) indicates the true values of the power of these bikes, and we can immediately see that the majority of them have a power of under 50, with over 2500 falling into the 20-29 range alone. However, there are still some small numbers of bikes that exceed that, some with a power of close to 200.

Extracting brand names from model names column to be used in later graphs

```{r}
brand_model <- bike_data$model_name 
testing <- str_split_fixed(brand_model, " ", 2)
brand_name <- testing[,1]
brand_name = str_replace_all(brand_name, "Royal", "Royal Enfield")
brand_df <- data.frame(brand_name)

bike_data_new <- cbind(bike_data, brand_df)
```


Bar plot showing relationship between brands and number of owners

```{r}
ggplot(data = bike_data_new) +
  geom_bar(mapping = aes(fill = brand_name, x = owner))
```

> "Owner" is a discrete category found in bike_data and "brand_name" is a discrete category that has been appended to the data by extracting existing values.
This barplot illustrates the relationship between the brand/make of the bikes and how many owners they've had. While the categories for "third owner" and "fourth owner or more" are too small to be discernible, this plot shows us that Royal Enfield, Bajaj, Honda and Yamaha are the most popular brands for bikes for both first and second owners, which could potentially be attributed to their longevity.


## Part 3 - Hypothesis Testing

```{r }
data <- read.csv("Cleaned_bike_data.csv")
str(data)
```

### Testing First hand owner price higher the Second hand owner (One-side test)

\begin{align*}
H_0: \mu_{first\_owner\_price}-\mu_{second\_owner\_price} & = 0 \\
H_1: \mu_{first\_owner\_price}-\mu_{second\_owner\_price} & > 0 
\end{align*}

```{r , fig.width=8, fig.height=7}


#split first hand owner and second hand owner
first_hand_owner <- subset(data, owner=="first owner")
second_hand_owner <- subset(data, owner=="second owner")


first_second_owner <- rbind(first_hand_owner, second_hand_owner)


ggplot(first_second_owner, aes(x=owner, y=price, fill=owner)) + 
    geom_boxplot(alpha=0.8) +
    theme(legend.position="none") +
    scale_fill_brewer(palette="Dark2") +
    coord_flip()
```
#### Don't know variance. So, use T-testing but need to check variance whether or not equal. \
#### Check variance equal or not \

\begin{align*}
H_0: \sigma_{first\_owner\_price}^2 = \sigma_{second\_owner\_price}^2 \\
H_1:  \sigma_{first\_owner\_price}^2 \ne \sigma_{second\_owner\_price}^2 
\end{align*}

```{r , fig.width=8, fig.height=7}
var.test(first_hand_owner$price, second_hand_owner$price, alternative = "two.sided")
#Not reject H0 => variance equal

#t-testing
t.test(first_hand_owner$price, second_hand_owner$price,alternative="greater", var.equal = TRUE, conf.level = 0.95)

#plot(t.test(first_hand_owner$price, second_hand_owner$price,alternative="greater", var.equal = TRUE, conf.level = 0.95))

```
\
>Should not reject Mu(first_price)=Mu(second_price) 


### Testing Price of Second hand bike not be different from third and later (Two-side test) \

\begin{align*}
H_0: \mu_{second\_owner\_price}-\mu_{later\_second\_owner\_price} & = 0 \\
H_1: \mu_{second\_owner\_price}-\mu_{later\_second\_owner\_price} & > 0 
\end{align*}


```{r, fig.width=8, fig.height=7}
second_hand_owner <- subset(data, owner=="second owner")
later_second_hand_owner <- subset(data, owner =="third owner" | owner == "fourth owner or more")

later_second_hand_owner$owner[later_second_hand_owner$owner == "third owner" | later_second_hand_owner$owner == "fourth owner or more"]<- "third owner or more"

second_later_owner <- rbind(second_hand_owner, later_second_hand_owner)


ggplot(second_later_owner, aes(x=owner, y=price, fill=owner)) + 
    geom_boxplot(alpha=0.8) +
    theme(legend.position="none") +
    scale_fill_brewer(palette="Dark2") +
    coord_flip()
```


#### Don't know variance. So, use T-testing but need to check variance whether or not equal. \
#### Check variance equal or not \
\begin{align*}
H_0: \sigma_{second\_owner\_price}^2 = \sigma_{later\_second\_owner\_price}^2 \\
H_1:  \sigma_{second\_owner\_price}^2 \ne \sigma_{later\_second\_owner\_price}^2 
\end{align*}

```{r, fig.width=8, fig.height=7}
var.test(second_hand_owner$price, later_second_hand_owner$price, alternative = "two.sided")
#reject H0 => variance not equal

#t-testing
t.test(second_hand_owner$price, later_second_hand_owner$price,alternative="two.sided", var.equal = FALSE, conf.level = 0.95)

#plot(t.test(second_hand_owner$price, later_second_hand_owner$price,alternative="two.sided", var.equal = FALSE, conf.level = 0.95))
```
\
> Should reject Mu(second_price)=Mu(later_second_price) => (Mu(second_price) != Mu(later_second_price)


### Testing Higher median kms driven price cheaper than lower median kms driven price (One side test)

\begin{align*}
H_0: \mu_{greater\_median\_kms\_driven}-\mu_{less\_median\_kms\_driven} & = 0 \\
H_1: \mu_{greater\_median\_kms\_driven}-\mu_{less\_median\_kms\_driven} & > 0
\end{align*}

```{r,  fig.width=8, fig.height=7}
median(data$kms_driven)


greater_median_kms_driven <- subset(data, kms_driven > 18000)
less_median_kms_driven <- subset(data, kms_driven <= 18000)

greater_median_kms_driven["median_kms_driven"] <- "greater"
less_median_kms_driven["median_kms_driven"] <- "less"

all_kms_driven <- rbind(greater_median_kms_driven, less_median_kms_driven)


ggplot(all_kms_driven, aes(x=median_kms_driven, y=price, fill=median_kms_driven)) + 
    geom_boxplot(alpha=0.8) +
    theme(legend.position="none") +
    scale_fill_brewer(palette="Dark2") +
    coord_flip()

```

#### Don't know variance. So, use T-testing but need to check variance whether or not equal.
#### Check variance equal or not
\begin{align*}
H_0: \sigma_{greater\_median\_kms\_driven\_price}^2 = \sigma_{less\_median\_kms\_driven\_price}^2 \\
H_1:  \sigma_{greater\_median\_kms\_driven\_price}^2 \ne \sigma_{less\_median\_kms\_driven\_price}^2 
\end{align*}

```{r, fig.width=8, fig.height=7}
var.test(greater_median_kms_driven$price, less_median_kms_driven$price , alternative = "two.sided")
# reject H0 => variance not equal

#t-testing
t.test(greater_median_kms_driven$price, less_median_kms_driven$price,alternative="less", var.equal = FALSE, conf.level = 0.95)

#plot(t.test(greater_median_kms_driven$price, less_median_kms_driven$price,alternative="less", var.equal = FALSE, conf.level = 0.95))
```
\
> Should reject H0 Mu(over_mean_kms_driven_price)=Mu(less_mean_kms_driven_price) => Mu(over_mean_kms_driven_price)< Mu(less_mean_kms_driven_price)




## Part 4 - Linear Regression

This section is for Linear Regression.

Including the necessary packages for regression
```{r}
library(MASS)
library(MLmetrics)
```

Setting the seed to store the data and keep it same.

```{r}
set.seed(101)
```

Splitting the Dataset into Training and Testing Dataset randomly. We have splitted the data in 70% and 30%.

```{r}
i = sample(2, nrow(bike_data), replace=TRUE, prob=c(0.7, 0.3))
bikeTraining <- bike_data[i==1,]
bikeTesting <- bike_data[i==2,]
```
The Training Dataset consists of 3554 entries and the Testing dataset consists of 1508 entries. 

# Model 1 - Forward Propogation Model.

We start with constructing the intercept model. The intercept is used to form a linear regression model with a constant variable. The full model is used to select all the attributes which are seen in the data table. We then use the stepAIC function to travel step by step and select all the elements with the highest AIC until the occurrence of the null variable. 

```{r}
intercept_model <- lm(price ~ 1, data = bikeTraining[,1:8])
full_model <- lm(price ~ .-model_name, data = bikeTraining[,1:8])
forward_model <- stepAIC(intercept_model, direction = "forward",scope = formula(full_model))
```

As we can see that the full model is constructed without the use of the model name. The reason for not including the model name is that it is not revelant to the price. 


```{r}
summary(forward_model)
forward_model$anova
```

### We can calculate the MAE and MSE for both the backward model as follows:

```{r}
forward_pred <-predict(object = forward_model, newdata = bikeTesting[,1:8])
MAE(y_pred = forward_pred, y_true = bikeTesting$price)
MSE(y_pred = forward_pred, y_true = bikeTesting$price)
```

### Plotting the Forward Propogation Model

```{r}
par(mfrow=c(2,2))
plot(forward_model)
```

# Model 2 - Backward Propogation

```{r}
backward <- stepAIC(full_model, direction = "backward")
summary(backward)
```

> We see that the accuracy of the backward model is similar to that of the forward model. 

### Calculation of the MAE and MSE for the backward model 

```{r}
backward_pred <-predict(object = backward, newdata = bikeTesting[,1:8])
MAE(y_pred = backward_pred, y_true = bikeTesting$price)
MSE(y_pred = backward_pred, y_true = bikeTesting$price)
```

### Plotting the Backward Propogation model 

```{r}
par(mfrow = c(2,2))
plot(backward)
```


> We can see that both the models have the same accuracy and same mean square error and Mean absolute error. The accuracy for both the models is 77.02%.
